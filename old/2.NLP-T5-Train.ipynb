{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_metric\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf52630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "df = pd.read_parquet('../data/parquet/dataset.parquet')\n",
    "labels = df['target'].unique().tolist()\n",
    "\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "lconv = ClassLabel(num_classes=len(labels), names=labels)\n",
    "df['target'] = df['target'].astype(str)\n",
    "\n",
    "train = df.sample(frac=0.90, random_state=42).reset_index(drop=True)\n",
    "test = df.drop(train.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ca68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for test purposes\n",
    "#train = train.sample(10).reset_index(drop=True)\n",
    "#test = test.sample(2).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fa21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(train)\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d0596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig\n",
    "model_name = 't5-base' \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"/mnt/dmif-nas/SMDC/HF-Cache/\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=\"/mnt/dmif-nas/SMDC/HF-Cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target\"], max_length=max_target_length, truncation=True, padding=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = ds_train.map(preprocess_function, batched=True)\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "ds_test = Dataset.from_pandas(test)\n",
    "tokenized_val = ds_test.map(preprocess_function, batched=True)\n",
    "tokenized_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c2223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "training_metrics = []\n",
    "\n",
    "def string2int(x):\n",
    "    try:\n",
    "        return lconv.str2int(x)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Replace -100 in the predictions as we can't decode them.\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_preds = [string2int(x) for x in decoded_preds]\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = [string2int(x) for x in decoded_labels]\n",
    "\n",
    "    precision = load_metric(\"precision\")\n",
    "    recall = load_metric(\"recall\")\n",
    "    accuracy = load_metric(\"accuracy\")\n",
    "    f1 = load_metric(\"f1\")\n",
    "\n",
    "    metrics = {\n",
    "        **precision.compute(predictions=decoded_preds, references=decoded_labels, average='macro'),\n",
    "        **recall.compute(predictions=decoded_preds, references=decoded_labels, average='macro'),\n",
    "        **accuracy.compute(predictions=decoded_preds, references=decoded_labels),\n",
    "        **f1.compute(predictions=decoded_preds, references=decoded_labels, average='macro'),\n",
    "        **{\"Not valid\": len([x for x in decoded_preds if x == -1])/len(decoded_preds)}\n",
    "    }\n",
    "\n",
    "    training_metrics.append(metrics)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/mnt/dmif-nas/SMDC/HF-tmp/',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    no_cuda=False,\n",
    "    per_device_train_batch_size=2,\n",
    ")\n",
    "\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         input_ids = inputs.get(\"img\")\n",
    "#         labels = inputs.get(\"labels\")\n",
    "#         logits = model(input_ids)\n",
    "#         y_pred = torch.max(logits, 1).indices.float()\n",
    "#         y_true = labels.view(-1).float()\n",
    "#         loss_fct = nn.CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits, labels)\n",
    "#         outputs = {'logits':y_pred, 'labels':y_true}\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model: torch.nn.Module, inputs, prediction_loss_only, ignore_keys):\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs.get(\"input_ids\").to('cuda')\n",
    "            attention_mask = inputs.get(\"attention_mask\").to('cuda')\n",
    "            labels = inputs.get(\"labels\").to('cuda')\n",
    "            beam_outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                early_stopping=True,\n",
    "                num_beams=1,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "            return (None, beam_outputs, labels)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset= tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "model_path = \"../models/t5\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "with open(f'../logs/t5-metrics_{datetime.now().strftime(\"%Y%m%d%H%M\")}.json', 'w') as f:\n",
    "    json.dump(training_metrics, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
