{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from src.utils import metrics_scores\n",
    "\n",
    "label2id = {'COMPLETED': 1, 'FAILED': 0}\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../out/parquet/raw.parquet')\n",
    "df = df[['playerId', 'startTime', 'state', 'counterName', 'target', 'periodTarget']]\n",
    "df.rename(columns={\n",
    "    'playerId': 'unique_id',\n",
    "    'startTime': 'ds',\n",
    "    'state': 'y'   \n",
    "}, inplace=True)\n",
    "df['y'] = df['y'].map(label2id)\n",
    "\n",
    "counterName_onehot = pd.get_dummies(df['counterName'], prefix='counterName', dtype=np.int8)\n",
    "df.drop(columns=['counterName'], inplace=True)\n",
    "df = df.join(counterName_onehot)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('unique_id').size().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_df(df):\n",
    "    ids = df['unique_id'].unique()\n",
    "    static_df = pd.get_dummies(ids, dtype=int, prefix='unique_id')\n",
    "    stat_exog_list = static_df.columns.tolist()\n",
    "    static_df['unique_id'] = ids\n",
    "    return static_df, stat_exog_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast \n",
    "from neuralforecast.models import MLP, TFT, GRU, NHITS, LSTM\n",
    "\n",
    "from neuralforecast.losses.pytorch import DistributionLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df, stat_exog_list = get_static_df(df)\n",
    "\n",
    "args = {\n",
    "    'h': 2,\n",
    "    'input_size': 6,\n",
    "    'loss': DistributionLoss('Bernoulli'),\n",
    "    'max_steps': 500,\n",
    "    'scaler_type': 'robust',\n",
    "    'futr_exog_list': [*counterName_onehot.columns.to_list(), 'target',\t'periodTarget'],\n",
    "    'stat_exog_list': stat_exog_list,\n",
    "    'start_padding_enabled': True\n",
    "}\n",
    "\n",
    "models = [\n",
    "    MLP(hidden_size=64, **args),\n",
    "    NHITS(**args),\n",
    "    TFT(**args),\n",
    "    GRU(\n",
    "        h=args['h'],\n",
    "        input_size=args['input_size'],\n",
    "        inference_input_size=args['input_size'],\n",
    "        loss=DistributionLoss('Normal'),\n",
    "        max_steps=args['max_steps'],\n",
    "        scaler_type=args['scaler_type'],\n",
    "        futr_exog_list=args['futr_exog_list'],\n",
    "        stat_exog_list=args['stat_exog_list'],\n",
    "    ),\n",
    "    LSTM(\n",
    "        h=args['h'],\n",
    "        input_size=args['input_size'],\n",
    "        inference_input_size=args['input_size'],\n",
    "        loss=DistributionLoss('Normal'),\n",
    "        max_steps=args['max_steps'],\n",
    "        scaler_type=args['scaler_type'],\n",
    "        futr_exog_list=args['futr_exog_list'],\n",
    "        stat_exog_list=args['stat_exog_list'],\n",
    "    )\n",
    "]\n",
    "\n",
    "nf = NeuralForecast(models=models, freq='W')\n",
    "Y_hat_df = nf.cross_validation(df=df, static_df=static_df, step_size=2, n_windows=10, refit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "    \n",
    "for m in nf.models:\n",
    "    m_name = str(m)\n",
    "    loss_dist = m.loss.distribution\n",
    "    threshold = Y_hat_df[m_name].mean() if loss_dist != 'Bernoulli' else 0.5\n",
    "    Y_hat_df[m_name] = (Y_hat_df[m_name] > threshold).astype(int)\n",
    "\n",
    "    metrics.append(Y_hat_df.groupby('ds').apply(lambda x: metrics_scores(x['y'], x[m_name], 1) | {'model': m_name}, include_groups=False))\n",
    "\n",
    "metrics = pd.concat(metrics)\n",
    "metrics = pd.DataFrame(metrics.tolist(), columns=['accuracy', 'precision', 'recall', 'f1', 'model'], index=metrics.index).reset_index()\n",
    "\n",
    "metrics.to_csv('../../out/csv/baseline_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
